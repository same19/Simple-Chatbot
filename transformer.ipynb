{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/build-your-own-transformer-from-scratch-using-pytorch-84c850470dcb\n",
    "\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
    "\n",
    "https://github.com/pytorch/examples/blob/main/word_language_model/model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import math\n",
    "import copy\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer import TransformerModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6908"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from net import Net_CBOW\n",
    "import numpy as np\n",
    "version = \"april27_WT2_nodatalim_10epoch_128dim_100minf\"\n",
    "\n",
    "# vocab = torch.load(f\"saves/vocab_{version}.pt\")\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(vocab, \"saves/vocab_may1_WT2_transformer_min25f.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6908"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "wikitext2 = load_dataset(\"wikitext\", \"wikitext-2-v1\")\n",
    "\n",
    "DATA_SPLIT = \"train\"\n",
    "text = wikitext2[DATA_SPLIT]['text']\n",
    "text = [item.lower().strip() for item in text if len(item) > 0]\n",
    "text = [item.split(\" \") + [\"\\n\"] for item in text if \"=\" not in item]\n",
    "# Prepare Corpus\n",
    "DATA_LIMIT = None #paragraph limit\n",
    "all_words = []\n",
    "for paragraph in text[:DATA_LIMIT]:\n",
    "    all_words += paragraph\n",
    "all_words = pd.Series(all_words)\n",
    "len(all_words)\n",
    "len(all_words.unique())\n",
    "\n",
    "v_counts = all_words.value_counts()\n",
    "\n",
    "#filter out rare words\n",
    "N_times = 25 #shows up in 100 different paragraphs\n",
    "# v_counts = all_words.value_counts()\n",
    "corpus = pd.Series([key for key in v_counts.keys() if v_counts[key] >= N_times])\n",
    "corpus\n",
    "vocab = {}\n",
    "for i in range(len(corpus)):\n",
    "    vocab[corpus[i]] = i\n",
    "len(vocab)\n",
    "#6908"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = torch.load(\"saves/vocab_may1_WT2_transformer_min25f.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lookup_id(word, vocab=vocab):\n",
    "    if word not in vocab:\n",
    "        return vocab[\"<unk>\"]\n",
    "    return vocab[word]\n",
    "def lookup_token(word_id, vocab=vocab):\n",
    "    for word in vocab:\n",
    "        if vocab[word] == word_id:\n",
    "            return word\n",
    "    return \"<unk>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['5', '6', '7', '8', '9', '8', '9', 'a', 'b']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def split_list(l, item, include_item = False):\n",
    "    particular_value = item\n",
    "    result = []\n",
    "    temp_list = []\n",
    "    for i in l:\n",
    "        if i == particular_value:\n",
    "            if include_item:\n",
    "                temp_list.append(i)\n",
    "            result.append(temp_list)\n",
    "            temp_list = []\n",
    "        else:\n",
    "            temp_list.append(i)\n",
    "    result.append(temp_list)\n",
    "    return result\n",
    "split_list([\"5\", \"6\", \"7\", \"8\", \"9\", \"8\", \"9\", \"a\", \"b\"], \"c\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "wikitext2 = load_dataset(\"wikitext\", \"wikitext-103-v1\")\n",
    "text_train = wikitext2[\"train\"]['text']\n",
    "text_train = [item.lower().strip() for item in text_train if len(item) > 0]\n",
    "text_test = wikitext2[\"test\"]['text']\n",
    "text_test = [item.lower().strip() for item in text_test if len(item) > 0]\n",
    "# len(text_test)\n",
    "text_train = [item.split(\" \") + [\"\\n\"] for item in text_train if \"=\" not in item]\n",
    "text_test = [item.split(\" \") + [\"\\n\"] for item in text_test if \"=\" not in item]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_seq_length = 32\n",
    "max_seq_length = 128\n",
    "\n",
    "buffer = 32\n",
    "\n",
    "# x_train = [[lookup_id(word) for word in split_list(paragraph, \".\", True)[1]] for paragraph in text_train if len(split_list(paragraph, \".\", True))>=3]\n",
    "# y_train = [item[1:] for item in x_train]\n",
    "# x_train = [item[:-1] for item in x_train]\n",
    "\n",
    "# x_test = [[lookup_id(word) for word in split_list(paragraph, \".\", True)[1]] for paragraph in text_test if len(split_list(paragraph, \".\", True))>=3]\n",
    "# print(len(x_test))\n",
    "# y_test = [item[1:] for item in x_test]\n",
    "# x_test = [item[:-1] for item in x_test]\n",
    "def seq_length():\n",
    "    return random.randint(min_seq_length, max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "197943"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = []\n",
    "y_train = []\n",
    "batch_size = 40\n",
    "batch_n = 0\n",
    "batch_seq_length = seq_length()\n",
    "count = 0\n",
    "unk_id = lookup_id(\"<unk>\")\n",
    "unk_f_cutoff = 0.1\n",
    "next_batch = False\n",
    "for paragraph in text_train:\n",
    "    if next_batch and len(x_train) % batch_size == 0:\n",
    "        batch_n += 1\n",
    "        batch_seq_length = seq_length()\n",
    "        next_batch = False\n",
    "    index = min(buffer, (paragraph.index(\".\")+1) if '.' in paragraph else len(paragraph))\n",
    "    while index + batch_seq_length + 1 <= len(paragraph) and (not next_batch or len(x_train) % batch_size != 0):\n",
    "        portion = paragraph[index:index + batch_seq_length+1]\n",
    "        if sum([i not in vocab for i in portion])/len(portion) > unk_f_cutoff:\n",
    "            # print(\"continuing\", sum(np.array(portion)==\"<unk>\"), len(portion), portion)\n",
    "            index += batch_seq_length\n",
    "            continue\n",
    "        x_train.append([lookup_id(word) for word in portion[:-1]])\n",
    "        y_train.append([lookup_id(word) for word in portion[1:]])\n",
    "        index += batch_seq_length\n",
    "        next_batch = True\n",
    "    \n",
    "\n",
    "# x_train = [[lookup_id(word) for word in paragraph[buffer:max_seq_length+buffer+1]]  for paragraph in text_train if len(paragraph) >= max_seq_length + buffer+1]\n",
    "# y_train = [item[1:] for item in x_train]\n",
    "# x_train = [item[:-1] for item in x_train]\n",
    "\n",
    "x_train = x_train[:-1]\n",
    "y_train = y_train[:-1]\n",
    "len(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 504, 4948.575)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_max_seq_length = 128\n",
    "def test_seq_length():\n",
    "    return random.randint(min_seq_length, test_max_seq_length)\n",
    "x_test = [[lookup_id(word) for word in paragraph[buffer:test_seq_length()+buffer+1]] for paragraph in text_test if len(paragraph) >= test_max_seq_length + buffer+1]\n",
    "y_test = [item[1:] for item in x_test]\n",
    "x_test = [item[:-1] for item in x_test]\n",
    "len(x_test[0]), len(x_test), len(x_train)/40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "184849"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unk_id = lookup_id(\"<unk>\")\n",
    "unk_prop = np.array([sum(np.array(item)==unk_id)/len(item) for item in x_train])\n",
    "sum(unk_prop <= 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['valkyria', 'of', 'the', 'battlefield', '3', ')', ',', 'commonly', 'referred', 'to', 'as', 'valkyria', 'chronicles', 'iii', 'outside', 'japan', ',', 'is', 'a', 'tactical', 'role', '@-@', 'playing', 'video', 'game', 'developed', 'by', '<unk>', 'and', '<unk>', 'for', 'the', 'playstation', 'portable', '.', 'released', 'in', 'january', '2011', 'in', 'japan', ',', 'it', 'is', 'the', 'third', 'game', 'in', 'the', 'valkyria', 'series', '.', '<unk>', 'the', 'same', 'fusion', 'of', 'tactical', 'and', 'real', '@-@', 'time', 'gameplay', 'as', 'its', '<unk>', ',', 'the', 'story', 'runs', 'parallel', 'to', 'the', 'first', 'game', 'and', 'follows', 'the', '\"', '<unk>', '\"', ',', 'a', '<unk>', 'military', 'unit', 'serving', 'the', 'nation', 'of', '<unk>', 'during', 'the', 'second', '<unk>', 'war', 'who', 'perform', 'secret', 'black', 'operations', 'and', 'are', '<unk>', 'against', 'the', 'imperial', 'unit', '\"', '<unk>', '<unk>', '\"']\n",
      "['of', 'the', 'battlefield', '3', ')', ',', 'commonly', 'referred', 'to', 'as', 'valkyria', 'chronicles', 'iii', 'outside', 'japan', ',', 'is', 'a', 'tactical', 'role', '@-@', 'playing', 'video', 'game', 'developed', 'by', '<unk>', 'and', '<unk>', 'for', 'the', 'playstation', 'portable', '.', 'released', 'in', 'january', '2011', 'in', 'japan', ',', 'it', 'is', 'the', 'third', 'game', 'in', 'the', 'valkyria', 'series', '.', '<unk>', 'the', 'same', 'fusion', 'of', 'tactical', 'and', 'real', '@-@', 'time', 'gameplay', 'as', 'its', '<unk>', ',', 'the', 'story', 'runs', 'parallel', 'to', 'the', 'first', 'game', 'and', 'follows', 'the', '\"', '<unk>', '\"', ',', 'a', '<unk>', 'military', 'unit', 'serving', 'the', 'nation', 'of', '<unk>', 'during', 'the', 'second', '<unk>', 'war', 'who', 'perform', 'secret', 'black', 'operations', 'and', 'are', '<unk>', 'against', 'the', 'imperial', 'unit', '\"', '<unk>', '<unk>', '\"', '.']\n"
     ]
    }
   ],
   "source": [
    "print([lookup_token(i) for i in x_train[0]])\n",
    "print([lookup_token(i) for i in y_train[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train = x_train[6872:]\n",
    "# y_train = y_train[6872:]\n",
    "# x_test = x_test\n",
    "# y_test = y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_test[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer import TransformerModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gregengel/Documents/GitHub/Simple-Chatbot/.venv/lib/python3.9/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "v_size = len(vocab)\n",
    "d_model = 128\n",
    "num_heads = 8\n",
    "num_layers = 6\n",
    "d_feedfoward = 2048\n",
    "dropout = 0.5\n",
    "\n",
    "transformer = TransformerModel(ntoken = v_size, ninp = d_model, nhead = num_heads, nhid = d_feedfoward, nlayers = num_layers, dropout = dropout)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_warmup as warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batches per epoch: 44.532\n",
      "||||||||||||||||||||||||||||||||||||||||||||| -> Epoch: 1, Train Loss: 8.313358076122565, Test Loss: 7.904851235771033\n",
      "||||||||||||||||||||||||||||||||||||||||||||| -> Epoch: 2, Train Loss: 7.867592939440652, Test Loss: 7.457566364537351\n",
      "||||||||||||||||||||||||||||||||||||||||||||| -> Epoch: 3, Train Loss: 7.392045249356437, Test Loss: 6.908466071583813\n",
      "||||||||||||||||||||||||||||||||||||||||||||| -> Epoch: 4, Train Loss: 6.857980460872175, Test Loss: 6.391155748054967\n",
      "||||||||||||||||||||||||||||||||||||||||||||| -> Epoch: 5, Train Loss: 6.4821508071568745, Test Loss: 6.126567196615718\n",
      "||||||||||||||||||||||||||||||||||||||||||||| -> Epoch: 6, Train Loss: 6.317530802708113, Test Loss: 6.035394352376853\n",
      "||||||||||||||||||||||||||||||||||||||||||||| -> Epoch: 7, Train Loss: 6.235146408479401, Test Loss: 5.975288279224146\n",
      "||||||||||||||||||||||||||||||||||||||||||||| -> Epoch: 8, Train Loss: 6.167859269237883, Test Loss: 5.9150056886231335\n",
      "||||||||||||||||||||||||||||||||||||||||||||| -> Epoch: 9, Train Loss: 6.100122363760636, Test Loss: 5.858320756413573\n",
      "||||||||||||||||||||||||||||||||||||||||||||| -> Epoch: 10, Train Loss: 6.034622013555493, Test Loss: 5.803390071913794\n",
      "||||||||||||||||||||||||||||||||||||||||||||| -> Epoch: 11, Train Loss: 5.968715691603946, Test Loss: 5.745296346242305\n",
      "||||||||||||||||||||||||||||||||||||||||||||| -> Epoch: 12, Train Loss: 5.9035367181856335, Test Loss: 5.687780262463509\n",
      "||||||||||||||||||||||||||||||||||||||||||||| -> Epoch: 13, Train Loss: 5.841289258340544, Test Loss: 5.632973281774724\n",
      "||||||||||||||||||||||||||||||||||||||||||||| -> Epoch: 14, Train Loss: 5.783747513696314, Test Loss: 5.578803743044591\n",
      "||||||||||||||||||||||||||||||||||||||||||||| -> Epoch: 15, Train Loss: 5.729936022665322, Test Loss: 5.531964605276625\n",
      "||||||||||||||||||||||||||||||||||||||||||||| -> Epoch: 16, Train Loss: 5.682686384035287, Test Loss: 5.489196905962975\n",
      "||||||||||||||||||||||||||||||||||||||||||||| -> Epoch: 17, Train Loss: 5.639290516226045, Test Loss: 5.4476826960378695\n",
      "||||||||||||||||||||||||||||||||||||||||||||| -> Epoch: 18, Train Loss: 5.602874645885928, Test Loss: 5.4132815820466975\n",
      "||||||||||||||||||||||||||||||||||||||||||||| -> Epoch: 19, Train Loss: 5.569413319400742, Test Loss: 5.385647098678226\n",
      "||||||||||||||||||||||||||||||||||||||||||||| -> Epoch: 20, Train Loss: 5.538887571330127, Test Loss: 5.354231802028893\n",
      "||||||||||||||||||||||||||||||||||||||||||||| -> Epoch: 21, Train Loss: 5.512684168452604, Test Loss: 5.337379379647556\n",
      "||||||||||||||||||||||||||||||||||||||||||||| -> Epoch: 22, Train Loss: 5.489016556991451, Test Loss: 5.316024742254812\n",
      "||||||||||||||||||||||||||||||||||||||||||||| -> Epoch: 23, Train Loss: 5.465224241744581, Test Loss: 5.2926738824286215\n",
      "||||||||||||||||||||||||||||||||||||||||||||| -> Epoch: 24, Train Loss: 5.444988998273451, Test Loss: 5.274667605731443\n",
      "||||||||||||||||||||||||||||||||||||||||||||| -> Epoch: 25, Train Loss: 5.426086341614619, Test Loss: 5.26189230336895\n",
      "||||||||||||||||||||||||||||||||||||||||||||| -> Epoch: 26, Train Loss: 5.407522030206522, Test Loss: 5.241328013791908\n",
      "||||||||||||||||||||||||||||||||||||||||||||| -> Epoch: 27, Train Loss: 5.391356978019168, Test Loss: 5.2231408453732415\n",
      "||||||||||||||||||||||||||||||||||||||||||||| -> Epoch: 28, Train Loss: 5.3753643859351, Test Loss: 5.217841459487461\n",
      "||||||||||||||||||||||||||||||||||||||||||||| -> Epoch: 29, Train Loss: 5.360410365205975, Test Loss: 5.201808506904992\n",
      "||||||||||||||||||||||||||||||||||||||||||||| -> Epoch: 30, Train Loss: 5.346705883881924, Test Loss: 5.1900651169045835\n",
      "||||||||||||||||||||||||||||||||||||||||||||| -> Epoch: 31, Train Loss: 5.333916114149216, Test Loss: 5.177451340598607\n",
      "||||||||||||||||||||||||||||||||||||||||||||| -> Epoch: 32, Train Loss: 5.320546188650039, Test Loss: 5.164314742783688\n",
      "||||||||||||||||||||||||||||||||||||||||||||| -> Epoch: 33, Train Loss: 5.3074806232867875, Test Loss: 5.152450712437608\n",
      "||||||||||||||||||||||||||||||||||||||||||||| -> Epoch: 34, Train Loss: 5.295796841309361, Test Loss: 5.143857662416039\n",
      "||||||||||||||||||||||||||||||||||||||||||||| -> Epoch: 35, Train Loss: 5.2835790336252515, Test Loss: 5.142694995025176\n",
      "||||||||||||||||||||||||||||||||||||||||||||| -> Epoch: 36, Train Loss: 5.272533307524175, Test Loss: 5.125220696334644\n",
      "||||||||||||||||||||||||||||||||||||||||||||| -> Epoch: 37, Train Loss: 5.261952351592188, Test Loss: 5.129329319645294\n",
      "||||||||||||||||||||||||||||||||||||||||||||| -> Epoch: 38, Train Loss: 5.2514358593752055, Test Loss: 5.116638436759199\n",
      "||||||||||||||||||||||||||||||||||||||||||||| -> Epoch: 39, Train Loss: 5.242221749500829, Test Loss: 5.096970931603893\n",
      "||||||||||||||||||||||||||||||||||||||||||||| -> Epoch: 40, Train Loss: 5.231805888449452, Test Loss: 5.096433936985705\n",
      "||||||||||||||||||||||||||||||||||||||||||||| -> Epoch: 41, Train Loss: 5.222130422401556, Test Loss: 5.0873568331935095\n",
      "||||||||||||||||||||||||||||||||||||||||||||| -> Epoch: 42, Train Loss: 5.214623006948749, Test Loss: 5.085427146220442\n",
      "||||||||||||||||||||||||||||||||||||||||||||| -> Epoch: 43, Train Loss: 5.2048777727606375, Test Loss: 5.080400786927792\n",
      "||||||||||||||||||||||||||||||||||||||||||||| -> Epoch: 44, Train Loss: 5.195632408796577, Test Loss: 5.073214264926925\n",
      "||||||||||||||||||||||||||||||||||||||||||||| -> Epoch: 45, Train Loss: 5.188248611851111, Test Loss: 5.070425191279736\n",
      "||||||||||||||||||||||||||||||||||||||||||||| -> Epoch: 46, Train Loss: 5.18110352399883, Test Loss: 5.063636502122499\n",
      "||||||||||||||||||||||||||||||||||||||||||||| -> Epoch: 47, Train Loss: 5.174447388802115, Test Loss: 5.058640008112076\n",
      "||||||||||||||||||||||||||||||||||||||||||||| -> Epoch: 48, Train Loss: 5.16609112985638, Test Loss: 5.053060152921809\n",
      "||||||||||||||||||||||||||||||||||||||||||||| -> Epoch: 49, Train Loss: 5.160007843349105, Test Loss: 5.042937765121933\n",
      "||||||||||||||||||||||||||||||||||||||||||||| -> Epoch: 50, Train Loss: 5.152250618231101, Test Loss: 5.046439294318016\n"
     ]
    }
   ],
   "source": [
    "net_file = \"saves/model_transformer_may5_1130pm.pt\"\n",
    "# transformer = torch.load(\"saves/model_transformer_may4_0130pm.pt\")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9) #normal lr is 0.0001\n",
    "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2)\n",
    "warmup_period = 250\n",
    "warmup_scheduler = warmup.LinearWarmup(optimizer, warmup_period = warmup_period)\n",
    "\n",
    "transformer.train()\n",
    "\n",
    "num_batches = len(x_train) // batch_size\n",
    "BATCH_PRINT_SIZE = 100\n",
    "percent_data_per_epoch = 0.9\n",
    "\n",
    "print(\"Batches per epoch:\", num_batches/BATCH_PRINT_SIZE * percent_data_per_epoch)\n",
    "\n",
    "\n",
    "\n",
    "indices = list(range(num_batches))\n",
    "for epoch in range(50):\n",
    "    train_loss = 0\n",
    "    # x_train_copy = [x_train[indices[i]] for i in range(len(indices))]\n",
    "    # y_train_copy = [y_train[indices[i]] for i in range(len(indices))]\n",
    "    x_train_copy = [x_train[indices[i]*batch_size:(indices[i]+1)*batch_size] for i in range(len(indices))]\n",
    "    y_train_copy = [y_train[indices[i]*batch_size:(indices[i]+1)*batch_size] for i in range(len(indices))]\n",
    "    for batch in range(int(num_batches * percent_data_per_epoch)):\n",
    "        # source = torch.tensor(x_train_copy[batch*batch_size:(batch+1)*batch_size])  # (batch_size, seq_length)\n",
    "        # target = torch.tensor(y_train_copy[batch*batch_size:(batch+1)*batch_size])  # (batch_size, seq_length)\n",
    "        source = torch.tensor(x_train_copy[batch])  # (batch_size, seq_length)\n",
    "        target = torch.tensor(y_train_copy[batch])  # (batch_size, seq_length)\n",
    "        # print(source)\n",
    "        # print(target)\n",
    "        optimizer.zero_grad()\n",
    "        output = transformer(source)\n",
    "        output = output.view(-1, v_size)\n",
    "        loss = criterion(output, target.view(-1))\n",
    "        # print(output.shape)\n",
    "        # print(target.view(-1).shape)\n",
    "        # print(\"________\")\n",
    "        # print(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch % BATCH_PRINT_SIZE == 0:\n",
    "            print(\"|\", end=\"\")\n",
    "        train_loss += loss.item()\n",
    "    print(\" -> \", end='')\n",
    "    #eval\n",
    "    test_loss = 0\n",
    "    count_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for datax, datay in zip(x_test,y_test):\n",
    "            source = torch.tensor([datax])\n",
    "            target = torch.tensor([datay]).view(-1)\n",
    "            output = transformer(source)\n",
    "            output = output.view(-1, v_size)\n",
    "            test_loss += criterion(output, target).item() * len(datax)\n",
    "            count_loss += len(datax)\n",
    "    test_loss /= count_loss\n",
    "\n",
    "    train_loss /= int(num_batches * percent_data_per_epoch)\n",
    "    random.shuffle(indices)\n",
    "    print(f\"Epoch: {epoch+1}, Train Loss: {train_loss}, Test Loss: {test_loss}\")\n",
    "    # warmup_scheduler.step()\n",
    "    with warmup_scheduler.dampening():\n",
    "        if warmup_scheduler.last_step + 1 >= warmup_period:\n",
    "            lr_scheduler.step()\n",
    "torch.save(transformer, net_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0001]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_scheduler.get_last_lr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['card', '.', 'the', '<unk>', 'were', 'performed', 'on', 'the', 'fastra', 'ii', ',', 'the', 'fastra', 'i', ',', 'a', '<unk>', '@-@', 'core', '<unk>', '(', 'consisting', 'of', '<unk>', '<unk>', ')', ',', 'an', '<unk>', '<unk>', '<unk>', '<unk>', 'card', 'on', 'an', '<unk>', 'core', '<unk>', '<unk>', '<unk>', ',', 'and', 'on', 'an', '<unk>', 'core', '<unk>', '<unk>', '<unk>', 'itself', '.', 'the', 'fastra', 'ii', 'is', 'over', 'three', 'times', 'faster', 'than', 'the', 'fastra', 'i', 'in', '<unk>', '<unk>', 'reconstruction', 'speed', '.', 'although', 'the', 'fastra', 'ii', '<unk>', 'more', 'power', 'than', 'the', 'fastra', 'i', ',', 'it', \"'s\", 'nearly', '3', 'times', 'as', 'energy', '<unk>', 'as', 'the', 'fastra', 'i', ',', 'and', 'over', '300', 'times', 'as', 'energy', '<unk>', 'as', 'the', '<unk>', '@-@', 'core', '<unk>', '.', 'the', 'video', 'cards', 'run', 'at', '37', 'degrees', '<unk>', 'when', '<unk>', ',', 'and', 'at', '60', 'degrees', '<unk>', 'at', 'full', 'load', '.']\n",
      "['with', 'john', 'largest', ';', 'not', ':', '5', 'main', 'share', 'were', 'but', 'british', '@-@', 'had', '08', 'total', 'by', 'prince', 'tends', 'government', '14', 'of', 'them', 'road', 'appearances', 'were', 'including', 'irish', 'relegated', '@-@', '.', 'at', 'by', '24', 'earthquake', 'for', '\"', '.', 'destroyers', 'trapped', 'hebrew', 'membrane', '7', 'earthquake', '@-@', 'of', 'km', 'on', '.', 'mole', 'along', 'first', '@-@', 'system', 'one', 'parrot', 'months', 'among', 'are', 'conclusion', 'title', 'million', 'destroyed', 'september', 'partial', 'efforts', 'at', 'of', 'prison', 'he', 'first', ',', 'of', '(', 'destroyers', \"'s\", 'he', 'series', 'fiction', 'had', 'which', 'now', 'own', '1', '@.@', 'in', 'those', 'to', 'and', '2009', 'old', 'mammals', 'had', 'sinclair', '1896', 'blacks', 'walk', 'by', 'a', 'barker', 'because', 'well', 'largest', 'following', 'au', 'measures', ')', 'sean', 'first', 'made', 'electricity', 'has', 'chagas', 'of', 'of', 'for', 'been', 'between', 'isbn', 'others', 'a', 'mammals', 'has', 'by', 'september', 'million', 'around', 'they']\n"
     ]
    }
   ],
   "source": [
    "o = transformer(src_data[0:1])\n",
    "sm = np.array(torch.softmax(o, 1)[0].detach())\n",
    "ids = [list(v).index(max(v)) for v in sm]\n",
    "words = [lookup_token(i) for i in ids]\n",
    "print([lookup_token(i) for i in src_data[0]])\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you are a <unk> assistant . answer the following question .\n",
      "['in', 'it', '\"', 'he', 'on']\n",
      "[0.062019486, 0.050097946, 0.06209897, 0.055168714, 0.036025554]\n",
      "you are a <unk> assistant . answer the following question . on\n",
      "['a', '1', '3', 'august', '8']\n",
      "[0.028086705, 0.041982103, 0.016172329, 0.01959111, 0.016415324]\n",
      "you are a <unk> assistant . answer the following question . on august\n",
      "['6', '5', '2', '1', '8']\n",
      "[0.026780343, 0.029847339, 0.02555999, 0.032186054, 0.025736617]\n",
      "you are a <unk> assistant . answer the following question . on august 6\n",
      "['@.@', ',', '.', '@-@', '@,@']\n",
      "[0.06803521, 0.07280752, 0.028437186, 0.037841234, 0.05639462]\n",
      "you are a <unk> assistant . answer the following question . on august 6 ,\n",
      "['but', 'which', '\"', 'he', 'and']\n",
      "[0.03431506, 0.028054025, 0.026013533, 0.033768643, 0.13182275]\n",
      "you are a <unk> assistant . answer the following question . on august 6 , which\n",
      "['were', ',', '.', 'was', 'had']\n",
      "[0.060836777, 0.035716124, 0.025741752, 0.056733724, 0.02901606]\n",
      "you are a <unk> assistant . answer the following question . on august 6 , which was\n",
      "['not', 'also', 'a', 'used', 'been']\n",
      "[0.02870853, 0.02546307, 0.031290825, 0.010490488, 0.013722667]\n",
      "you are a <unk> assistant . answer the following question . on august 6 , which was a\n",
      "['result', 'tropical', 'new', '\"', 'second']\n",
      "[0.008644181, 0.008595868, 0.011227255, 0.011243199, 0.008105037]\n",
      "you are a <unk> assistant . answer the following question . on august 6 , which was a second\n",
      "['to', ',', '.', 'of', '@-@']\n",
      "[0.015082641, 0.03149005, 0.014526706, 0.028784756, 0.08860873]\n",
      "you are a <unk> assistant . answer the following question . on august 6 , which was a second to\n",
      "['have', 'be', 'a', 'make', 'do']\n",
      "[0.014901284, 0.064500295, 0.031057967, 0.0139460545, 0.014360462]\n",
      "you are a <unk> assistant . answer the following question . on august 6 , which was a second to do\n",
      "['it', ',', 'that', 'a', 'his']\n",
      "[0.03209362, 0.023035316, 0.028227646, 0.06979177, 0.021529889]\n",
      "you are a <unk> assistant . answer the following question . on august 6 , which was a second to do that\n",
      "['had', 'he', '\"', 'was', 'they']\n",
      "[0.039076947, 0.036942977, 0.02465964, 0.07089173, 0.02648889]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: '?'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 26\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28mprint\u001b[39m(words)\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28mprint\u001b[39m(top5p[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m---> 26\u001b[0m     chosen_word \u001b[38;5;241m=\u001b[39m words[\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mChoose a word: \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     27\u001b[0m     text\u001b[38;5;241m.\u001b[39mappend(lookup_id(chosen_word))\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# text\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: '?'"
     ]
    }
   ],
   "source": [
    "text = \"you are a helpful assistant . Answer the following question . \"\n",
    "\n",
    "text = [lookup_id(word.lower()) for word in text.strip().split(\" \")]\n",
    "\n",
    "for count in range(50):\n",
    "    i = torch.tensor([text])\n",
    "    o = transformer(i)\n",
    "    sm = np.array(torch.softmax(o, 2)[0].detach())\n",
    "    top5 = [np.zeros(5) for _ in range(len(sm))]\n",
    "    top5p = [np.zeros(5) for _ in range(len(sm))]\n",
    "    for vi in range(len(sm)-1,len(sm)):\n",
    "        v = sm[vi]\n",
    "        for item in v:\n",
    "            m = top5[vi][list(top5[vi]).index(min(top5[vi]))]\n",
    "            if lookup_token(list(v).index(item)) != \"<unk>\":\n",
    "                top5[vi][list(top5[vi]).index(min(top5[vi]))] = max(m, item)\n",
    "        top5[vi] = [list(v).index(i) for i in top5[vi]]\n",
    "        top5p[vi] = [v[i] for i in top5[vi]]\n",
    "    # ids = [list(v).index(max(v)) for v in sm]\n",
    "    words = [[lookup_token(i) for i in w] for w in top5][-1]\n",
    "    # print([lookup_token(i) for i in src_data[0]])\n",
    "    # print(words)\n",
    "    print(' '.join([lookup_token(i) for i in text]))\n",
    "    print(words)\n",
    "    print(top5p[-1])\n",
    "    chosen_word = words[int(input(\"Choose a word: \"))-1]\n",
    "    text.append(lookup_id(chosen_word))\n",
    "# text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[267]])\n",
      "| Generated 0/['i', 'am', 'a', 'big', 'baseball', 'fan', '.', 'i', 'love', 'the', '<unk>', 'and', 'the', 'last'] words\n",
      "tensor([[49]])\n",
      "| Generated 1/['i', 'am', 'a', 'big', 'baseball', 'fan', '.', 'i', 'love', 'the', '<unk>', 'and', 'the', 'last', 'two'] words\n",
      "tensor([[2173]])\n",
      "| Generated 2/['i', 'am', 'a', 'big', 'baseball', 'fan', '.', 'i', 'love', 'the', '<unk>', 'and', 'the', 'last', 'two', 'decades'] words\n",
      "tensor([[2]])\n",
      "| Generated 3/['i', 'am', 'a', 'big', 'baseball', 'fan', '.', 'i', 'love', 'the', '<unk>', 'and', 'the', 'last', 'two', 'decades', '.'] words\n",
      "tensor([[9]])\n",
      "| Generated 4/['i', 'am', 'a', 'big', 'baseball', 'fan', '.', 'i', 'love', 'the', '<unk>', 'and', 'the', 'last', 'two', 'decades', '.', '\"'] words\n",
      "tensor([[1]])\n",
      "| Generated 5/['i', 'am', 'a', 'big', 'baseball', 'fan', '.', 'i', 'love', 'the', '<unk>', 'and', 'the', 'last', 'two', 'decades', '.', '\"', ','] words\n",
      "tensor([[5]])\n",
      "| Generated 6/['i', 'am', 'a', 'big', 'baseball', 'fan', '.', 'i', 'love', 'the', '<unk>', 'and', 'the', 'last', 'two', 'decades', '.', '\"', ',', 'and'] words\n",
      "tensor([[3]])\n",
      "| Generated 7/['i', 'am', 'a', 'big', 'baseball', 'fan', '.', 'i', 'love', 'the', '<unk>', 'and', 'the', 'last', 'two', 'decades', '.', '\"', ',', 'and', 'of'] words\n",
      "tensor([[35]])\n",
      "| Generated 8/['i', 'am', 'a', 'big', 'baseball', 'fan', '.', 'i', 'love', 'the', '<unk>', 'and', 'the', 'last', 'two', 'decades', '.', '\"', ',', 'and', 'of', 'their'] words\n",
      "tensor([[277]])\n",
      "| Generated 9/['i', 'am', 'a', 'big', 'baseball', 'fan', '.', 'i', 'love', 'the', '<unk>', 'and', 'the', 'last', 'two', 'decades', '.', '\"', ',', 'and', 'of', 'their', 'own'] words\n",
      "tensor([[694]])\n",
      "| Generated 10/['i', 'am', 'a', 'big', 'baseball', 'fan', '.', 'i', 'love', 'the', '<unk>', 'and', 'the', 'last', 'two', 'decades', '.', '\"', ',', 'and', 'of', 'their', 'own', 'review'] words\n",
      "tensor([[3]])\n",
      "| Generated 11/['i', 'am', 'a', 'big', 'baseball', 'fan', '.', 'i', 'love', 'the', '<unk>', 'and', 'the', 'last', 'two', 'decades', '.', '\"', ',', 'and', 'of', 'their', 'own', 'review', 'of'] words\n",
      "tensor([[8]])\n",
      "| Generated 12/['i', 'am', 'a', 'big', 'baseball', 'fan', '.', 'i', 'love', 'the', '<unk>', 'and', 'the', 'last', 'two', 'decades', '.', '\"', ',', 'and', 'of', 'their', 'own', 'review', 'of', 'a'] words\n",
      "tensor([[264]])\n",
      "| Generated 13/['i', 'am', 'a', 'big', 'baseball', 'fan', '.', 'i', 'love', 'the', '<unk>', 'and', 'the', 'last', 'two', 'decades', '.', '\"', ',', 'and', 'of', 'their', 'own', 'review', 'of', 'a', 'way'] words\n",
      "tensor([[27]])\n",
      "| Generated 14/['i', 'am', 'a', 'big', 'baseball', 'fan', '.', 'i', 'love', 'the', '<unk>', 'and', 'the', 'last', 'two', 'decades', '.', '\"', ',', 'and', 'of', 'their', 'own', 'review', 'of', 'a', 'way', 'he'] words\n",
      "tensor([[30]])\n",
      "| Generated 15/['i', 'am', 'a', 'big', 'baseball', 'fan', '.', 'i', 'love', 'the', '<unk>', 'and', 'the', 'last', 'two', 'decades', '.', '\"', ',', 'and', 'of', 'their', 'own', 'review', 'of', 'a', 'way', 'he', 'had'] words\n",
      "tensor([[52]])\n",
      "| Generated 16/['i', 'am', 'a', 'big', 'baseball', 'fan', '.', 'i', 'love', 'the', '<unk>', 'and', 'the', 'last', 'two', 'decades', '.', '\"', ',', 'and', 'of', 'their', 'own', 'review', 'of', 'a', 'way', 'he', 'had', 'been'] words\n",
      "tensor([[779]])\n",
      "| Generated 17/['i', 'am', 'a', 'big', 'baseball', 'fan', '.', 'i', 'love', 'the', '<unk>', 'and', 'the', 'last', 'two', 'decades', '.', '\"', ',', 'and', 'of', 'their', 'own', 'review', 'of', 'a', 'way', 'he', 'had', 'been', 'suggested'] words\n",
      "tensor([[15]])\n",
      "| Generated 18/['i', 'am', 'a', 'big', 'baseball', 'fan', '.', 'i', 'love', 'the', '<unk>', 'and', 'the', 'last', 'two', 'decades', '.', '\"', ',', 'and', 'of', 'their', 'own', 'review', 'of', 'a', 'way', 'he', 'had', 'been', 'suggested', 'that'] words\n",
      "tensor([[27]])\n",
      "| Generated 19/['i', 'am', 'a', 'big', 'baseball', 'fan', '.', 'i', 'love', 'the', '<unk>', 'and', 'the', 'last', 'two', 'decades', '.', '\"', ',', 'and', 'of', 'their', 'own', 'review', 'of', 'a', 'way', 'he', 'had', 'been', 'suggested', 'that', 'he'] words\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "ntokens = len(vocab)\n",
    "input = torch.randint(ntokens, (1, 1), dtype=torch.long).to(device)\n",
    "model = transformer\n",
    "temperature = 0.5\n",
    "log_interval = 1\n",
    "input = 'i am a big baseball fan . i love the sights and the '\n",
    "input = [lookup_id(i) for i in input.strip().split(\" \")]\n",
    "input = torch.tensor(input).view(len(input), 1)\n",
    "\n",
    "\n",
    "with open('out_generation.txt', 'w') as outf:\n",
    "    with torch.no_grad():  # no tracking history\n",
    "        for i in range(20):\n",
    "            output = model(input, False)\n",
    "            word_weights = output[-1].squeeze().div(temperature).exp().cpu()\n",
    "            word_weights[lookup_id(\"<unk>\")] = 0\n",
    "            word_idx = torch.multinomial(word_weights, 1)[0]\n",
    "            word_tensor = torch.Tensor([[word_idx]]).long().to(device)\n",
    "            print(word_tensor)\n",
    "            input = torch.cat([input, word_tensor], 0)\n",
    "\n",
    "            word = lookup_token(word_idx)\n",
    "\n",
    "            outf.write(word + ('\\n' if i % 20 == 19 else ' '))\n",
    "\n",
    "            if i % log_interval == 0:\n",
    "                print('| Generated {}/{} words'.format(i, [lookup_token(i[0])for i in input]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
