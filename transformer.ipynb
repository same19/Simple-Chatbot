{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/build-your-own-transformer-from-scratch-using-pytorch-84c850470dcb\n",
    "\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
    "\n",
    "https://github.com/pytorch/examples/blob/main/word_language_model/model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import math\n",
    "import copy\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer import Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2156"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from net import Net_CBOW\n",
    "import numpy as np\n",
    "version = \"april27_WT2_nodatalim_10epoch_128dim_100minf\"\n",
    "\n",
    "vocab = torch.load(f\"saves/vocab_{version}.pt\")\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lookup_id(word, vocab=vocab):\n",
    "    if word not in vocab:\n",
    "        return vocab[\"<unk>\"]\n",
    "    return vocab[word]\n",
    "def lookup_token(word_id, vocab=vocab):\n",
    "    for word in vocab:\n",
    "        if vocab[word] == word_id:\n",
    "            return word\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "wikitext2 = load_dataset(\"wikitext\", \"wikitext-2-v1\")\n",
    "text_train = wikitext2[\"train\"]['text']\n",
    "text_train = [item.lower().strip() for item in text_train if len(item) > 0]\n",
    "text_test = wikitext2[\"test\"]['text']\n",
    "text_test = [item.lower().strip() for item in text_test if len(item) > 0]\n",
    "len(text_test)\n",
    "text_train = [item.split(\" \") + [\"\\n\"] for item in text_train if \"=\" not in item]\n",
    "text_test = [item.split(\" \") + [\"\\n\"] for item in text_test if \"=\" not in item]\n",
    "\n",
    "max_seq_length = 128\n",
    "start_i = 20\n",
    "\n",
    "x_train = [[lookup_id(word) for word in paragraph[start_i:max_seq_length]] for paragraph in text_train if len(paragraph) >= max_seq_length + start_i]\n",
    "y_train = [[word for word in paragraph] for paragraph in x_train]\n",
    "x_test = [[lookup_id(word) for word in paragraph[start_i:max_seq_length]] for paragraph in text_test if len(paragraph) >= max_seq_length + start_i]\n",
    "y_test = [[word for word in paragraph] for paragraph in x_test]\n",
    "# print([[word for word in paragraph[start_i:max_seq_length]] for paragraph in text_train if len(paragraph) >= max_seq_length + start_i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5319\n",
      "128605 574452 0.22387423144144333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5319"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_vocab_size = len(vocab)\n",
    "tgt_vocab_size = len(vocab)\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "num_layers = 6\n",
    "d_ff = 2048\n",
    "max_seq_length = 128\n",
    "dropout = 0.1\n",
    "\n",
    "transformer = Transformer(src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout)\n",
    "\n",
    "print(len(x_train))\n",
    "x_train = x_train\n",
    "y_train = y_train\n",
    "src_data = torch.tensor(x_train)\n",
    "\n",
    "unk_id = lookup_id(\"<unk>\")\n",
    "count_unk = sum(sum((i == unk_id) for i in paragraph) for paragraph in src_data).item()\n",
    "count_total = sum(sum(1 for i in paragraph) for paragraph in src_data)\n",
    "print(count_unk, count_total, count_unk/count_total)\n",
    "\n",
    "# a = torch.randint(1, src_vocab_size, (64, max_seq_length))\n",
    "# print(a[:, 1:])\n",
    "# print(a[:, :-1])\n",
    "len(x_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batches per epoch: 53\n",
      "|||||||||||||||||||||||||| -> Epoch: 1, Loss: 5.632079216150137\n",
      "|||||||||||||||||||||||||| -> Epoch: 2, Loss: 5.072457240178035\n",
      "|||||||||||||||||||||||||| -> Epoch: 3, Loss: 4.915947877443754\n",
      "|||||||||||||||||||||||||| -> Epoch: 4, Loss: 4.761176622830904\n",
      "|||||||||||||||||||||||||| -> Epoch: 5, Loss: 4.634160188528208\n",
      "|||||||||||||||||||||||||| -> Epoch: 6, Loss: 4.515117828662579\n",
      "|||||||||||||||||||||||||| -> Epoch: 7, Loss: 4.42727437386146\n",
      "|||||||||||||||||||||||||| -> Epoch: 8, Loss: 4.333838829627404\n",
      "|||||||||||||||||||||||||| -> Epoch: 9, Loss: 4.249075962946965\n",
      "|||||||||||||||||||||||||| -> Epoch: 10, Loss: 4.1785673728356\n",
      "|||||||||||||||||||||||||| -> Epoch: 11, Loss: 4.112065058488112\n",
      "|||||||||||||||||||||||||| -> Epoch: 12, Loss: 3.9986279744368334\n",
      "|||||||||||||||||||||||||| -> Epoch: 13, Loss: 3.891067523222703\n",
      "|||||||||||||||||||||||||| -> Epoch: 14, Loss: 3.7791364192962646\n",
      "|||||||||||||||||||||||||| -> Epoch: 15, Loss: 3.6912831709935117\n",
      "||||||||"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[111], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m output \u001b[38;5;241m=\u001b[39m transformer(src_data, tgt_data[:, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     23\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, tgt_vocab_size), tgt_data[:, \u001b[38;5;241m1\u001b[39m:]\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m---> 24\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m|\u001b[39m\u001b[38;5;124m\"\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/Code/Simple Chatbot/.venv/lib/python3.11/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Code/Simple Chatbot/.venv/lib/python3.11/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "transformer.train()\n",
    "\n",
    "batch_size = 100\n",
    "num_batches = len(x_train) // batch_size\n",
    "print(\"Batches per epoch:\", num_batches)\n",
    "\n",
    "percent_data_per_epoch = 0.5\n",
    "\n",
    "\n",
    "indices = list(range(len(x_train)))\n",
    "for epoch in range(20):\n",
    "    epoch_loss = 0\n",
    "    x_train_copy = [x_train[indices[i]] for i in range(len(indices))]\n",
    "    y_train_copy = [y_train[indices[i]] for i in range(len(indices))]\n",
    "    for batch in range(int(num_batches * percent_data_per_epoch)):\n",
    "        src_data = torch.tensor(x_train_copy[batch*batch_size:(batch+1)*batch_size])  # (batch_size, seq_length)\n",
    "        tgt_data = torch.tensor(y_train_copy[batch*batch_size:(batch+1)*batch_size])  # (batch_size, seq_length)\n",
    "        optimizer.zero_grad()\n",
    "        output = transformer(src_data, tgt_data[:, :-1])\n",
    "        loss = criterion(output.contiguous().view(-1, tgt_vocab_size), tgt_data[:, 1:].contiguous().view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(\"|\", end=\"\")\n",
    "        epoch_loss += loss.item()\n",
    "    epoch_loss /= int(num_batches * percent_data_per_epoch)\n",
    "    random.shuffle(indices)\n",
    "    print(f\" -> Epoch: {epoch+1}, Loss: {epoch_loss}\")\n",
    "\n",
    "torch.save(transformer, \"saves/model_transformer_apr29_1200pm.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(transformer, \"saves/model_transformer_apr29_1200pm.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<unk>', '<unk>', '<unk>', '(', 'jack', '<unk>', ')', 'and', '<unk>', '(', 'james', '<unk>', '<unk>', ')', '.', 'the', 'captain', '<unk>', 'that', 'he', 'was', 'on', 'the', '<unk>', '<unk>', ',', 'returning', 'from', '<unk>', 'space', 'with', 'the', '<unk>', '<unk>', 'of', 'time', '.', 'they', 'had', '<unk>', 'up', 'a', '<unk>', 'on', 'the', 'way', ',', 'a', 'human', 'called', '<unk>', '<unk>', '.', '<unk>', 'the', 'ship', 'found', 'itself', 'some', '200', 'light', 'years', 'away', 'from', 'its', 'previous', 'location', 'and', 'a', 'hundred', 'years', 'in', 'the', 'past', ',', 'near', 'deep', 'space', 'station', '<unk>', 'and', 'found', 'the', '<unk>', '<unk>', 'in', '<unk>', '.', 'they', '<unk>', 'that', 'the', '<unk>', '@-@', '<unk>', 'was', '<unk>', '<unk>', '(', '<unk>', '<unk>', ')', ',', 'a', '<unk>', '<unk>', 'who', 'had']\n",
      "['march', 'earliest', '@-@', 'yard', 'minor', '(', '.', 'according', '0', 'km', 'mexico', 'first', 'ft', 'operated', 'km', ')', 'been', 'longer', '@.@', \"'s\", 'it', 'continued', 'has', 'now', 'that', 'it', 'main', ',', ',', 'it', \"'s\", ',', 'but', 'to', 'be', 'weeks', ',', ',', 'but', 'and', 'others', 'him', 'their', ',', 'to', 'provide', 'york', ',', 'including', 'not', 'him', 'storm', 'on', 'for', 'according', 'period', 'is', 'now', 'for', 'be', 'rest', 'for', 'for', 'example', 'latter', 'of', 'mexico', 'and', 'and', 'thus', 'from', 'which', 'can', 'able', 'from', 'create', 'been', 'from', 'a', 'few', 'days', '@-@', 'with', 'october', '@-@', 'yard', 'and', 'then', '.', '.', 'however', 'same', 'stated', 'are', 'scientology', 'book', \"'s\", 'now', 'by', 'those', 'that', 'year', 'and', 'many', 'that', 'been', 'in']\n"
     ]
    }
   ],
   "source": [
    "o = transformer(src_data[1:2], tgt_data[1:2, :-1])\n",
    "sm = np.array(torch.softmax(o, 1)[0].detach())\n",
    "ids = [list(v).index(max(v)) for v in sm]\n",
    "words = [lookup_token(i) for i in ids]\n",
    "print([lookup_token(i) for i in src_data[0]])\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you are a <unk> <unk> . <unk> the following <unk> .\n",
      "['after', 'it', 'are', 'a', 'this']\n",
      "[0.008957216, 0.013795887, 0.01188761, 0.11148329, 0.016574614]\n",
      "you are a <unk> <unk> . <unk> the following <unk> . a\n",
      "['a', 'few', 'large', 'small', '\"']\n",
      "[0.014941752, 0.004584193, 0.005560547, 0.005065698, 0.0033910982]\n",
      "you are a <unk> <unk> . <unk> the following <unk> . a few\n",
      "['can', 'a', '.', 'are', 'is']\n",
      "[0.0043566437, 0.009275039, 0.0067396043, 0.048805617, 0.008312006]\n",
      "you are a <unk> <unk> . <unk> the following <unk> . a few can\n",
      "['have', 'are', 'do', 'a', 'be']\n",
      "[0.057717957, 0.035070132, 0.0101899505, 0.023004128, 0.12751704]\n",
      "you are a <unk> <unk> . <unk> the following <unk> . a few can have\n",
      "['a', 'been', 'also', 'are', 'have']\n",
      "[0.15154977, 0.12867396, 0.006671486, 0.024408525, 0.011477391]\n",
      "you are a <unk> <unk> . <unk> the following <unk> . a few can have a\n",
      "['small', 'a', 'large', 'are', 'few']\n",
      "[0.012695176, 0.0087218555, 0.007897064, 0.006415769, 0.008232827]\n",
      "you are a <unk> <unk> . <unk> the following <unk> . a few can have a large\n",
      "['are', 'have', '.', 'a', 'is']\n",
      "[0.12945305, 0.010292493, 0.00744872, 0.02018076, 0.014980078]\n",
      "you are a <unk> <unk> . <unk> the following <unk> . a few can have a large .\n",
      "['they', 'are', 'this', 'after', 'a']\n",
      "[0.019157127, 0.017090844, 0.031466667, 0.014925685, 0.32724714]\n",
      "you are a <unk> <unk> . <unk> the following <unk> . a few can have a large . they\n",
      "['also', 'found', 'have', 'are', 'can']\n",
      "[0.008092646, 0.009088459, 0.02662814, 0.33317173, 0.010756265]\n",
      "you are a <unk> <unk> . <unk> the following <unk> . a few can have a large . they also\n",
      "['been', 'a', 'have', 'are', 'also']\n",
      "[0.021032123, 0.02904975, 0.023048567, 0.04869975, 0.013803414]\n",
      "you are a <unk> <unk> . <unk> the following <unk> . a few can have a large . they also have\n",
      "['also', 'been', 'are', 'a', 'have']\n",
      "[0.011146491, 0.17580204, 0.011320784, 0.21023068, 0.011357635]\n",
      "you are a <unk> <unk> . <unk> the following <unk> . a few can have a large . they also have been\n",
      "['used', 'are', 'also', 'found', 'a']\n",
      "[0.007852544, 0.014396482, 0.008581804, 0.028955208, 0.14886618]\n",
      "you are a <unk> <unk> . <unk> the following <unk> . a few can have a large . they also have been found\n",
      "['they', 'a', '.', 'are', 'that']\n",
      "[0.01727171, 0.35511142, 0.021520322, 0.0510977, 0.024303798]\n",
      "you are a <unk> <unk> . <unk> the following <unk> . a few can have a large . they also have been found .\n",
      "['are', 'there', 'they', 'this', 'a']\n",
      "[0.015704252, 0.021700885, 0.09130804, 0.036192864, 0.107594654]\n",
      "you are a <unk> <unk> . <unk> the following <unk> . a few can have a large . they also have been found . they\n",
      "['have', 'also', 'can', 'are', 'could']\n",
      "[0.060890276, 0.0140260095, 0.0130752055, 0.34680888, 0.008936084]\n",
      "you are a <unk> <unk> . <unk> the following <unk> . a few can have a large . they also have been found . they also\n",
      "['a', 'can', 'been', 'have', 'are']\n",
      "[0.08272033, 0.011403698, 0.02964887, 0.03986354, 0.08075382]\n",
      "you are a <unk> <unk> . <unk> the following <unk> . a few can have a large . they also have been found . they also have\n",
      "['are', 'have', 'also', 'been', 'a']\n",
      "[0.013841829, 0.016521351, 0.011069539, 0.20542043, 0.17427863]\n",
      "you are a <unk> <unk> . <unk> the following <unk> . a few can have a large . they also have been found . they also have a\n",
      "['small', 'more', 'large', 'few', 'are']\n",
      "[0.01935141, 0.009512894, 0.013155225, 0.014721647, 0.014153689]\n",
      "you are a <unk> <unk> . <unk> the following <unk> . a few can have a large . they also have been found . they also have a more\n",
      "['a', 'are', '.', 'they', 'than']\n",
      "[0.007050143, 0.010846202, 0.010540224, 0.004020272, 0.11074463]\n",
      "you are a <unk> <unk> . <unk> the following <unk> . a few can have a large . they also have been found . they also have a more than\n",
      "['they', 'a', '.', 'their', 'are']\n",
      "[0.019155845, 0.24513638, 0.024536887, 0.012115843, 0.021601006]\n",
      "you are a <unk> <unk> . <unk> the following <unk> . a few can have a large . they also have been found . they also have a more than their\n",
      "['they', 'own', '.', 'first', 'are']\n",
      "[0.0035166335, 0.018602308, 0.0073338416, 0.0044018645, 0.006253538]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: ''",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[151], line 26\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28mprint\u001b[39m(words)\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28mprint\u001b[39m(top5p[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m---> 26\u001b[0m     chosen_word \u001b[38;5;241m=\u001b[39m words[\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mChoose a word: \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     27\u001b[0m     text\u001b[38;5;241m.\u001b[39mappend(lookup_id(chosen_word))\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# text\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: ''"
     ]
    }
   ],
   "source": [
    "text = \"you are a helpful assistant . Answer the following question . \"\n",
    "\n",
    "text = [lookup_id(word.lower()) for word in text.strip().split(\" \")]\n",
    "\n",
    "for count in range(50):\n",
    "    i = torch.tensor([text])\n",
    "    o = transformer(i, i)\n",
    "    sm = np.array(torch.softmax(o, 2)[0].detach())\n",
    "    top5 = [np.zeros(5) for _ in range(len(sm))]\n",
    "    top5p = [np.zeros(5) for _ in range(len(sm))]\n",
    "    for vi in range(len(sm)-1,len(sm)):\n",
    "        v = sm[vi]\n",
    "        for item in v:\n",
    "            m = top5[vi][list(top5[vi]).index(min(top5[vi]))]\n",
    "            if lookup_token(list(v).index(item)) != \"<unk>\":\n",
    "                top5[vi][list(top5[vi]).index(min(top5[vi]))] = max(m, item)\n",
    "        top5[vi] = [list(v).index(i) for i in top5[vi]]\n",
    "        top5p[vi] = [v[i] for i in top5[vi]]\n",
    "    # ids = [list(v).index(max(v)) for v in sm]\n",
    "    words = [[lookup_token(i) for i in w] for w in top5][-1]\n",
    "    # print([lookup_token(i) for i in src_data[0]])\n",
    "    # print(words)\n",
    "    print(' '.join([lookup_token(i) for i in text]))\n",
    "    print(words)\n",
    "    print(top5p[-1])\n",
    "    chosen_word = words[int(input(\"Choose a word: \"))-1]\n",
    "    text.append(lookup_id(chosen_word))\n",
    "# text\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
