{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/build-your-own-transformer-from-scratch-using-pytorch-84c850470dcb\n",
    "\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
    "\n",
    "https://github.com/pytorch/examples/blob/main/word_language_model/model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import math\n",
    "import copy\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer import TransformerModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6908"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from net import Net_CBOW\n",
    "import numpy as np\n",
    "version = \"april27_WT2_nodatalim_10epoch_128dim_100minf\"\n",
    "\n",
    "# vocab = torch.load(f\"saves/vocab_{version}.pt\")\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(vocab, \"saves/vocab_may1_WT2_transformer_min25f.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6908"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "wikitext2 = load_dataset(\"wikitext\", \"wikitext-2-v1\")\n",
    "\n",
    "DATA_SPLIT = \"train\"\n",
    "text = wikitext2[DATA_SPLIT]['text']\n",
    "text = [item.lower().strip() for item in text if len(item) > 0]\n",
    "text = [item.split(\" \") + [\"\\n\"] for item in text if \"=\" not in item]\n",
    "# Prepare Corpus\n",
    "DATA_LIMIT = None #paragraph limit\n",
    "all_words = []\n",
    "for paragraph in text[:DATA_LIMIT]:\n",
    "    all_words += paragraph\n",
    "all_words = pd.Series(all_words)\n",
    "len(all_words)\n",
    "len(all_words.unique())\n",
    "\n",
    "v_counts = all_words.value_counts()\n",
    "\n",
    "#filter out rare words\n",
    "N_times = 25 #shows up in 100 different paragraphs\n",
    "# v_counts = all_words.value_counts()\n",
    "corpus = pd.Series([key for key in v_counts.keys() if v_counts[key] >= N_times])\n",
    "corpus\n",
    "vocab = {}\n",
    "for i in range(len(corpus)):\n",
    "    vocab[corpus[i]] = i\n",
    "len(vocab)\n",
    "#6908"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = torch.load(\"saves/vocab_may1_WT2_transformer_min25f.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lookup_id(word, vocab=vocab):\n",
    "    if word not in vocab:\n",
    "        return vocab[\"<unk>\"]\n",
    "    return vocab[word]\n",
    "def lookup_token(word_id, vocab=vocab):\n",
    "    for word in vocab:\n",
    "        if vocab[word] == word_id:\n",
    "            return word\n",
    "    return \"<unk>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['5', '6', '7', '8', '9', '8', '9', 'a', 'b']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def split_list(l, item, include_item = False):\n",
    "    particular_value = item\n",
    "    result = []\n",
    "    temp_list = []\n",
    "    for i in l:\n",
    "        if i == particular_value:\n",
    "            if include_item:\n",
    "                temp_list.append(i)\n",
    "            result.append(temp_list)\n",
    "            temp_list = []\n",
    "        else:\n",
    "            temp_list.append(i)\n",
    "    result.append(temp_list)\n",
    "    return result\n",
    "split_list([\"5\", \"6\", \"7\", \"8\", \"9\", \"8\", \"9\", \"a\", \"b\"], \"c\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gregengel/Documents/GitHub/Simple-Chatbot/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/gregengel/Documents/GitHub/Simple-Chatbot/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading readme: 100%|██████████| 10.5k/10.5k [00:00<00:00, 14.4MB/s]\n",
      "Downloading data: 100%|██████████| 722k/722k [00:00<00:00, 1.83MB/s]\n",
      "Downloading data: 100%|██████████| 156M/156M [00:04<00:00, 37.2MB/s] \n",
      "Downloading data: 100%|██████████| 156M/156M [00:03<00:00, 39.6MB/s] \n",
      "Downloading data: 100%|██████████| 655k/655k [00:00<00:00, 3.65MB/s]\n",
      "Generating test split: 100%|██████████| 4358/4358 [00:00<00:00, 451428.14 examples/s]\n",
      "Generating train split: 100%|██████████| 1801350/1801350 [00:00<00:00, 3552481.01 examples/s]\n",
      "Generating validation split: 100%|██████████| 3760/3760 [00:00<00:00, 1644653.57 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "wikitext2 = load_dataset(\"wikitext\", \"wikitext-103-v1\")\n",
    "text_train = wikitext2[\"train\"]['text']\n",
    "text_train = [item.lower().strip() for item in text_train if len(item) > 0]\n",
    "text_test = wikitext2[\"test\"]['text']\n",
    "text_test = [item.lower().strip() for item in text_test if len(item) > 0]\n",
    "# len(text_test)\n",
    "text_train = [item.split(\" \") + [\"\\n\"] for item in text_train if \"=\" not in item]\n",
    "text_test = [item.split(\" \") + [\"\\n\"] for item in text_test if \"=\" not in item]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-->\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(100, 106, 43871)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "max_seq_length = 128\n",
    "buffer = 128\n",
    "\n",
    "# x_train = [[lookup_id(word) for word in split_list(paragraph, \".\", True)[1]] for paragraph in text_train if len(split_list(paragraph, \".\", True))>=3]\n",
    "# y_train = [item[1:] for item in x_train]\n",
    "# x_train = [item[:-1] for item in x_train]\n",
    "\n",
    "# x_test = [[lookup_id(word) for word in split_list(paragraph, \".\", True)[1]] for paragraph in text_test if len(split_list(paragraph, \".\", True))>=3]\n",
    "# print(len(x_test))\n",
    "# y_test = [item[1:] for item in x_test]\n",
    "# x_test = [item[:-1] for item in x_test]\n",
    "def seq_length():\n",
    "    return random.randint(5, max_seq_length)\n",
    "\n",
    "x_train = [[lookup_id(word) for word in paragraph[buffer:max_seq_length+buffer+1]] for paragraph in text_train if len(paragraph) >= max_seq_length + buffer+1]\n",
    "y_train = [item[1:] for item in x_train]\n",
    "x_train = [item[:-1] for item in x_train]\n",
    "print(\"-->\")\n",
    "x_test = [[lookup_id(word) for word in paragraph[buffer:seq_length()+buffer+1]] for paragraph in text_test if len(paragraph) >= max_seq_length + buffer+1]\n",
    "y_test = [item[1:] for item in x_test]\n",
    "x_test = [item[:-1] for item in x_test]\n",
    "len(x_test[0]), len(x_test), len(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'modern', '<unk>', 'of', '<unk>', '<unk>', ',', '<unk>', 'el', '<unk>', ',', '<unk>', '<unk>', ',', '<unk>', 'said', 'and', '<unk>', '<unk>', '(', 'ancient', '<unk>', ')', 'were', 'constructed', 'to', '<unk>', 'the', 'city', \"'s\", 'defence', '.', '<unk>', 'said', 'was', 'the', 'base', 'for', 'the', '<unk>', 'iv', '<unk>', '<unk>', ',', 'an', '<unk>', 'cavalry', 'unit', 'from', '<unk>', ',', 'while', '<unk>', '<unk>', 'housed', 'spanish', 'and', '<unk>', '<unk>', '.', '<unk>', '<unk>', 'was', 'the', 'location', 'of', 'a', '<unk>', 'of', '<unk>', ',', 'and', '<unk>', 'and', '<unk>', 'cavalry', 'were', 'based', 'at', '<unk>', '.', 'rising', 'tensions', 'in', 'the', 'region', 'near', 'the', 'end', 'of', 'the', '2nd', 'century', 'led', 'the', 'emperor', '<unk>', '<unk>', 'to', 'order', 'the', 'construction', 'of', 'a', '2', '@.@', '5', 'km', '(', '1', '@.@', '6', 'mi', ')', 'circuit', 'of', 'walls', 'with', 'eight', 'gates', 'and', '40', 'towers', '.', '<unk>', 'was', 'connected', 'by']\n",
      "['modern', '<unk>', 'of', '<unk>', '<unk>', ',', '<unk>', 'el', '<unk>', ',', '<unk>', '<unk>', ',', '<unk>', 'said', 'and', '<unk>', '<unk>', '(', 'ancient', '<unk>', ')', 'were', 'constructed', 'to', '<unk>', 'the', 'city', \"'s\", 'defence', '.', '<unk>', 'said', 'was', 'the', 'base', 'for', 'the', '<unk>', 'iv', '<unk>', '<unk>', ',', 'an', '<unk>', 'cavalry', 'unit', 'from', '<unk>', ',', 'while', '<unk>', '<unk>', 'housed', 'spanish', 'and', '<unk>', '<unk>', '.', '<unk>', '<unk>', 'was', 'the', 'location', 'of', 'a', '<unk>', 'of', '<unk>', ',', 'and', '<unk>', 'and', '<unk>', 'cavalry', 'were', 'based', 'at', '<unk>', '.', 'rising', 'tensions', 'in', 'the', 'region', 'near', 'the', 'end', 'of', 'the', '2nd', 'century', 'led', 'the', 'emperor', '<unk>', '<unk>', 'to', 'order', 'the', 'construction', 'of', 'a', '2', '@.@', '5', 'km', '(', '1', '@.@', '6', 'mi', ')', 'circuit', 'of', 'walls', 'with', 'eight', 'gates', 'and', '40', 'towers', '.', '<unk>', 'was', 'connected', 'by', 'road']\n"
     ]
    }
   ],
   "source": [
    "print([lookup_token(i) for i in x_train[6871]])\n",
    "print([lookup_token(i) for i in y_train[6871]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train = x_train[6872:]\n",
    "# y_train = y_train[6872:]\n",
    "# x_test = x_test\n",
    "# y_test = y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_test[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer import TransformerModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43871\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "106"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_size = len(vocab)\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "num_layers = 6\n",
    "d_feedfoward = 2048\n",
    "dropout = 0.5\n",
    "\n",
    "# transformer = TransformerModel(ntoken = v_size, ninp = d_model, nhead = num_heads, nhid = d_feedfoward, nlayers = num_layers, dropout = dropout)\n",
    "\n",
    "print(len(x_train))\n",
    "x_train = x_train\n",
    "y_train = y_train\n",
    "# src_data = torch.tensor(x_train)\n",
    "\n",
    "# unk_id = lookup_id(\"<unk>\")\n",
    "# count_unk = sum(sum((i == unk_id) for i in paragraph) for paragraph in x_train).item()\n",
    "# count_total = sum(sum(1 for i in paragraph) for paragraph in x_train)\n",
    "# print(count_unk, count_total, count_unk/count_total)\n",
    "\n",
    "# a = torch.randint(1, src_vocab_size, (64, max_seq_length))\n",
    "# print(a[:, 1:])\n",
    "# print(a[:, :-1])\n",
    "len(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batches per epoch: 10.96\n",
      "||||||||||| -> Epoch: 1, Train Loss: 4.890634160523021, Test Loss: 4.874810701081103\n",
      "||||||||||| -> Epoch: 2, Train Loss: 4.857419503938168, Test Loss: 4.871288404276558\n",
      "||||||||||| -> Epoch: 3, Train Loss: 4.850252295852801, Test Loss: 4.8706667832041575\n",
      "||||||||||| -> Epoch: 4, Train Loss: 4.856415892959735, Test Loss: 4.866820785955238\n",
      "||||||||||| -> Epoch: 5, Train Loss: 4.847893697406174, Test Loss: 4.871889695237375\n",
      "||||||||||| -> Epoch: 6, Train Loss: 4.845654093891109, Test Loss: 4.868001178120248\n",
      "||||||||||| -> Epoch: 7, Train Loss: 4.851339939537398, Test Loss: 4.870087774525057\n",
      "||||||||||| -> Epoch: 8, Train Loss: 4.859588802407641, Test Loss: 4.871078146287463\n",
      "||||||||||| -> Epoch: 9, Train Loss: 4.852017232037466, Test Loss: 4.872375854306939\n",
      "||||||||||| -> Epoch: 10, Train Loss: 4.85011849709607, Test Loss: 4.872102461242835\n",
      "||||||||||| -> Epoch: 11, Train Loss: 4.85286436605891, Test Loss: 4.872820693982447\n",
      "||||||||||| -> Epoch: 12, Train Loss: 4.858846891910658, Test Loss: 4.873394102292937\n",
      "||||||||||| -> Epoch: 13, Train Loss: 4.863003748272537, Test Loss: 4.871939610123087\n",
      "||||||||||| -> Epoch: 14, Train Loss: 4.853737835490375, Test Loss: 4.871561596190514\n",
      "||||||||||| -> Epoch: 15, Train Loss: 4.846432257136073, Test Loss: 4.869826234876074\n",
      "||||||||||| -> Epoch: 16, Train Loss: 4.850338467764198, Test Loss: 4.870859035969245\n",
      "||||||||||| -> Epoch: 17, Train Loss: 4.854184452546846, Test Loss: 4.873848453037802\n",
      "||||||||||| -> Epoch: 18, Train Loss: 4.85721766620601, Test Loss: 4.870738883374283\n",
      "||||||||||| -> Epoch: 19, Train Loss: 4.850641425596464, Test Loss: 4.867178972121558\n",
      "||||||||||| -> Epoch: 20, Train Loss: 4.85502818308839, Test Loss: 4.8746804500331775\n",
      "||||||||||| -> Epoch: 21, Train Loss: 4.8421282155798115, Test Loss: 4.869559050898295\n",
      "||||||||||| -> Epoch: 22, Train Loss: 4.855495689112112, Test Loss: 4.868295831913516\n",
      "||||||||||| -> Epoch: 23, Train Loss: 4.849802463426503, Test Loss: 4.8713388024261635\n",
      "||||||||||| -> Epoch: 24, Train Loss: 4.845797092542735, Test Loss: 4.864380624934635\n",
      "||||||||||| -> Epoch: 25, Train Loss: 4.847608229435912, Test Loss: 4.872862768484616\n",
      "||||||||||| -> Epoch: 26, Train Loss: 4.850573894080766, Test Loss: 4.8728700861494225\n",
      "||||||||||| -> Epoch: 27, Train Loss: 4.857744186296376, Test Loss: 4.8702082735011025\n",
      "||||||||||| -> Epoch: 28, Train Loss: 4.849079608917236, Test Loss: 4.872349182599051\n",
      "||||||||||| -> Epoch: 29, Train Loss: 4.844149690155589, Test Loss: 4.867164718089409\n",
      "||||||||||| -> Epoch: 30, Train Loss: 4.852896922225252, Test Loss: 4.866215173645252\n",
      "||||||||||| -> Epoch: 31, Train Loss: 4.83953920854341, Test Loss: 4.869539762083252\n",
      "||||||||||| -> Epoch: 32, Train Loss: 4.845869759900855, Test Loss: 4.870433986890941\n",
      "||||||||||| -> Epoch: 33, Train Loss: 4.846901876117111, Test Loss: 4.869358045054058\n",
      "||||||||||| -> Epoch: 34, Train Loss: 4.845225154806714, Test Loss: 4.866593575070132\n",
      "||||||||||| -> Epoch: 35, Train Loss: 4.856250544206811, Test Loss: 4.8709474899246405\n",
      "||||||||||| -> Epoch: 36, Train Loss: 4.846667910934588, Test Loss: 4.870378232820507\n",
      "||||||||||| -> Epoch: 37, Train Loss: 4.848382647978056, Test Loss: 4.865416482304208\n",
      "||||||||||| -> Epoch: 38, Train Loss: 4.846366768583246, Test Loss: 4.869215509602307\n",
      "||||||||||| -> Epoch: 39, Train Loss: 4.853902961136004, Test Loss: 4.866078613399714\n",
      "|"
     ]
    }
   ],
   "source": [
    "net_file = \"saves/model_transformer_may4_0130pm.pt\"\n",
    "transformer = torch.load(\"saves/model_transformer_may4_1150am.pt\")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.Adam(transformer.parameters(), lr=0.000005, betas=(0.9, 0.98), eps=1e-9) #normal lr is 0.0001\n",
    "\n",
    "transformer.train()\n",
    "\n",
    "batch_size = 40\n",
    "num_batches = len(x_train) // batch_size\n",
    "BATCH_PRINT_SIZE = 10\n",
    "percent_data_per_epoch = 0.1\n",
    "\n",
    "print(\"Batches per epoch:\", num_batches/BATCH_PRINT_SIZE * percent_data_per_epoch)\n",
    "\n",
    "\n",
    "\n",
    "indices = list(range(len(x_train)))\n",
    "for epoch in range(100):\n",
    "    train_loss = 0\n",
    "    x_train_copy = [x_train[indices[i]] for i in range(len(indices))]\n",
    "    y_train_copy = [y_train[indices[i]] for i in range(len(indices))]\n",
    "    for batch in range(int(num_batches * percent_data_per_epoch)):\n",
    "        source = torch.tensor(x_train_copy[batch*batch_size:(batch+1)*batch_size])  # (batch_size, seq_length)\n",
    "        target = torch.tensor(y_train_copy[batch*batch_size:(batch+1)*batch_size])  # (batch_size, seq_length)\n",
    "        # print(source)\n",
    "        # print(target)\n",
    "        optimizer.zero_grad()\n",
    "        output = transformer(source)\n",
    "        output = output.view(-1, v_size)\n",
    "        loss = criterion(output, target.view(-1))\n",
    "        # print(output.shape)\n",
    "        # print(target.view(-1).shape)\n",
    "        # print(\"________\")\n",
    "        # print(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch % BATCH_PRINT_SIZE == 0:\n",
    "            print(\"|\", end=\"\")\n",
    "        train_loss += loss.item()\n",
    "    print(\" -> \", end='')\n",
    "    #eval\n",
    "    test_loss = 0\n",
    "    count_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for datax, datay in zip(x_test,y_test):\n",
    "            source = torch.tensor([datax])\n",
    "            target = torch.tensor([datay]).view(-1)\n",
    "            output = transformer(source)\n",
    "            output = output.view(-1, v_size)\n",
    "            test_loss += criterion(output, target).item() * len(datax)\n",
    "            count_loss += len(datax)\n",
    "    test_loss /= count_loss\n",
    "\n",
    "    train_loss /= int(num_batches * percent_data_per_epoch)\n",
    "    random.shuffle(indices)\n",
    "    print(f\"Epoch: {epoch+1}, Train Loss: {train_loss}, Test Loss: {test_loss}\")\n",
    "torch.save(transformer, net_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net_file = \"saves/model_transformer_may4_1150am.pt\"\n",
    "# torch.save(transformer, net_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['card', '.', 'the', '<unk>', 'were', 'performed', 'on', 'the', 'fastra', 'ii', ',', 'the', 'fastra', 'i', ',', 'a', '<unk>', '@-@', 'core', '<unk>', '(', 'consisting', 'of', '<unk>', '<unk>', ')', ',', 'an', '<unk>', '<unk>', '<unk>', '<unk>', 'card', 'on', 'an', '<unk>', 'core', '<unk>', '<unk>', '<unk>', ',', 'and', 'on', 'an', '<unk>', 'core', '<unk>', '<unk>', '<unk>', 'itself', '.', 'the', 'fastra', 'ii', 'is', 'over', 'three', 'times', 'faster', 'than', 'the', 'fastra', 'i', 'in', '<unk>', '<unk>', 'reconstruction', 'speed', '.', 'although', 'the', 'fastra', 'ii', '<unk>', 'more', 'power', 'than', 'the', 'fastra', 'i', ',', 'it', \"'s\", 'nearly', '3', 'times', 'as', 'energy', '<unk>', 'as', 'the', 'fastra', 'i', ',', 'and', 'over', '300', 'times', 'as', 'energy', '<unk>', 'as', 'the', '<unk>', '@-@', 'core', '<unk>', '.', 'the', 'video', 'cards', 'run', 'at', '37', 'degrees', '<unk>', 'when', '<unk>', ',', 'and', 'at', '60', 'degrees', '<unk>', 'at', 'full', 'load', '.']\n",
      "['with', 'john', 'largest', ';', 'not', ':', '5', 'main', 'share', 'were', 'but', 'british', '@-@', 'had', '08', 'total', 'by', 'prince', 'tends', 'government', '14', 'of', 'them', 'road', 'appearances', 'were', 'including', 'irish', 'relegated', '@-@', '.', 'at', 'by', '24', 'earthquake', 'for', '\"', '.', 'destroyers', 'trapped', 'hebrew', 'membrane', '7', 'earthquake', '@-@', 'of', 'km', 'on', '.', 'mole', 'along', 'first', '@-@', 'system', 'one', 'parrot', 'months', 'among', 'are', 'conclusion', 'title', 'million', 'destroyed', 'september', 'partial', 'efforts', 'at', 'of', 'prison', 'he', 'first', ',', 'of', '(', 'destroyers', \"'s\", 'he', 'series', 'fiction', 'had', 'which', 'now', 'own', '1', '@.@', 'in', 'those', 'to', 'and', '2009', 'old', 'mammals', 'had', 'sinclair', '1896', 'blacks', 'walk', 'by', 'a', 'barker', 'because', 'well', 'largest', 'following', 'au', 'measures', ')', 'sean', 'first', 'made', 'electricity', 'has', 'chagas', 'of', 'of', 'for', 'been', 'between', 'isbn', 'others', 'a', 'mammals', 'has', 'by', 'september', 'million', 'around', 'they']\n"
     ]
    }
   ],
   "source": [
    "o = transformer(src_data[0:1])\n",
    "sm = np.array(torch.softmax(o, 1)[0].detach())\n",
    "ids = [list(v).index(max(v)) for v in sm]\n",
    "words = [lookup_token(i) for i in ids]\n",
    "print([lookup_token(i) for i in src_data[0]])\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you are a <unk> assistant . answer the following question .\n",
      "['in', '\"', 'it', 'a', 'he']\n",
      "[0.068896994, 0.041094005, 0.047903307, 0.02770467, 0.05970837]\n",
      "you are a <unk> assistant . answer the following question . in\n",
      "['a', 'an', 'june', 'his', ')']\n",
      "[0.05232671, 0.012509325, 0.013340906, 0.019252285, 0.01443879]\n",
      "you are a <unk> assistant . answer the following question . in a\n",
      "['single', 'large', '\"', 'new', 'few']\n",
      "[0.010547135, 0.008964184, 0.024736026, 0.016492296, 0.010190338]\n",
      "you are a <unk> assistant . answer the following question . in a single\n",
      "['from', ',', '.', '@-@', '\"']\n",
      "[0.04512883, 0.13348018, 0.03855977, 0.04473776, 0.055649318]\n",
      "you are a <unk> assistant . answer the following question . in a single ,\n",
      "['but', 'a', 'which', '\"', 'and']\n",
      "[0.03510762, 0.030457802, 0.03479596, 0.025242856, 0.115802065]\n",
      "you are a <unk> assistant . answer the following question . in a single , a\n",
      "['\"', 'single', 'number', 'few', 'new']\n",
      "[0.013539589, 0.012328783, 0.0104001695, 0.008300327, 0.01911035]\n",
      "you are a <unk> assistant . answer the following question . in a single , a \"\n",
      "['in', ',', '.', 'a', 'and']\n",
      "[0.016372822, 0.08652402, 0.10754151, 0.022400826, 0.035081048]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: ''",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 26\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28mprint\u001b[39m(words)\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28mprint\u001b[39m(top5p[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m---> 26\u001b[0m     chosen_word \u001b[38;5;241m=\u001b[39m words[\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mChoose a word: \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     27\u001b[0m     text\u001b[38;5;241m.\u001b[39mappend(lookup_id(chosen_word))\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# text\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: ''"
     ]
    }
   ],
   "source": [
    "text = \"you are a helpful assistant . Answer the following question . \"\n",
    "\n",
    "text = [lookup_id(word.lower()) for word in text.strip().split(\" \")]\n",
    "\n",
    "for count in range(50):\n",
    "    i = torch.tensor([text])\n",
    "    o = transformer(i)\n",
    "    sm = np.array(torch.softmax(o, 2)[0].detach())\n",
    "    top5 = [np.zeros(5) for _ in range(len(sm))]\n",
    "    top5p = [np.zeros(5) for _ in range(len(sm))]\n",
    "    for vi in range(len(sm)-1,len(sm)):\n",
    "        v = sm[vi]\n",
    "        for item in v:\n",
    "            m = top5[vi][list(top5[vi]).index(min(top5[vi]))]\n",
    "            if lookup_token(list(v).index(item)) != \"<unk>\":\n",
    "                top5[vi][list(top5[vi]).index(min(top5[vi]))] = max(m, item)\n",
    "        top5[vi] = [list(v).index(i) for i in top5[vi]]\n",
    "        top5p[vi] = [v[i] for i in top5[vi]]\n",
    "    # ids = [list(v).index(max(v)) for v in sm]\n",
    "    words = [[lookup_token(i) for i in w] for w in top5][-1]\n",
    "    # print([lookup_token(i) for i in src_data[0]])\n",
    "    # print(words)\n",
    "    print(' '.join([lookup_token(i) for i in text]))\n",
    "    print(words)\n",
    "    print(top5p[-1])\n",
    "    chosen_word = words[int(input(\"Choose a word: \"))-1]\n",
    "    text.append(lookup_id(chosen_word))\n",
    "# text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2]])\n",
      "| Generated 0/['alongside', 'the', 'main', 'story', '<unk>', '.'] words\n",
      "tensor([[3131]])\n",
      "| Generated 1/['alongside', 'the', 'main', 'story', '<unk>', '.', 'parks'] words\n",
      "tensor([[6722]])\n",
      "| Generated 2/['alongside', 'the', 'main', 'story', '<unk>', '.', 'parks', 'logs'] words\n",
      "tensor([[4826]])\n",
      "| Generated 3/['alongside', 'the', 'main', 'story', '<unk>', '.', 'parks', 'logs', 'download'] words\n",
      "tensor([[3035]])\n",
      "| Generated 4/['alongside', 'the', 'main', 'story', '<unk>', '.', 'parks', 'logs', 'download', 'terminus'] words\n",
      "tensor([[673]])\n",
      "| Generated 5/['alongside', 'the', 'main', 'story', '<unk>', '.', 'parks', 'logs', 'download', 'terminus', 'post'] words\n",
      "tensor([[520]])\n",
      "| Generated 6/['alongside', 'the', 'main', 'story', '<unk>', '.', 'parks', 'logs', 'download', 'terminus', 'post', 'works'] words\n",
      "tensor([[28]])\n",
      "| Generated 7/['alongside', 'the', 'main', 'story', '<unk>', '.', 'parks', 'logs', 'download', 'terminus', 'post', 'works', 'were'] words\n",
      "tensor([[485]])\n",
      "| Generated 8/['alongside', 'the', 'main', 'story', '<unk>', '.', 'parks', 'logs', 'download', 'terminus', 'post', 'works', 'were', 'too'] words\n",
      "tensor([[2530]])\n",
      "| Generated 9/['alongside', 'the', 'main', 'story', '<unk>', '.', 'parks', 'logs', 'download', 'terminus', 'post', 'works', 'were', 'too', 'rarely'] words\n",
      "tensor([[129]])\n",
      "| Generated 10/['alongside', 'the', 'main', 'story', '<unk>', '.', 'parks', 'logs', 'download', 'terminus', 'post', 'works', 'were', 'too', 'rarely', 'four'] words\n",
      "tensor([[456]])\n",
      "| Generated 11/['alongside', 'the', 'main', 'story', '<unk>', '.', 'parks', 'logs', 'download', 'terminus', 'post', 'works', 'were', 'too', 'rarely', 'four', 'players'] words\n",
      "tensor([[1287]])\n",
      "| Generated 12/['alongside', 'the', 'main', 'story', '<unk>', '.', 'parks', 'logs', 'download', 'terminus', 'post', 'works', 'were', 'too', 'rarely', 'four', 'players', '300'] words\n",
      "tensor([[5562]])\n",
      "| Generated 13/['alongside', 'the', 'main', 'story', '<unk>', '.', 'parks', 'logs', 'download', 'terminus', 'post', 'works', 'were', 'too', 'rarely', 'four', 'players', '300', 'senate'] words\n",
      "tensor([[2816]])\n",
      "| Generated 14/['alongside', 'the', 'main', 'story', '<unk>', '.', 'parks', 'logs', 'download', 'terminus', 'post', 'works', 'were', 'too', 'rarely', 'four', 'players', '300', 'senate', 'believes'] words\n",
      "tensor([[2082]])\n",
      "| Generated 15/['alongside', 'the', 'main', 'story', '<unk>', '.', 'parks', 'logs', 'download', 'terminus', 'post', 'works', 'were', 'too', 'rarely', 'four', 'players', '300', 'senate', 'believes', 'entirely'] words\n",
      "tensor([[18]])\n",
      "| Generated 16/['alongside', 'the', 'main', 'story', '<unk>', '.', 'parks', 'logs', 'download', 'terminus', 'post', 'works', 'were', 'too', 'rarely', 'four', 'players', '300', 'senate', 'believes', 'entirely', 'with'] words\n",
      "tensor([[6198]])\n",
      "| Generated 17/['alongside', 'the', 'main', 'story', '<unk>', '.', 'parks', 'logs', 'download', 'terminus', 'post', 'works', 'were', 'too', 'rarely', 'four', 'players', '300', 'senate', 'believes', 'entirely', 'with', 'commodore'] words\n",
      "tensor([[1833]])\n",
      "| Generated 18/['alongside', 'the', 'main', 'story', '<unk>', '.', 'parks', 'logs', 'download', 'terminus', 'post', 'works', 'were', 'too', 'rarely', 'four', 'players', '300', 'senate', 'believes', 'entirely', 'with', 'commodore', 'broke'] words\n",
      "tensor([[63]])\n",
      "| Generated 19/['alongside', 'the', 'main', 'story', '<unk>', '.', 'parks', 'logs', 'download', 'terminus', 'post', 'works', 'were', 'too', 'rarely', 'four', 'players', '300', 'senate', 'believes', 'entirely', 'with', 'commodore', 'broke', 'all'] words\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "ntokens = len(vocab)\n",
    "input = torch.randint(ntokens, (1, 1), dtype=torch.long).to(device)\n",
    "model = transformer\n",
    "temperature = 2\n",
    "log_interval = 1\n",
    "input = 'alongside the main story <unk>'\n",
    "input = [lookup_id(i) for i in input.strip().split(\" \")]\n",
    "input = torch.tensor(input).view(len(input), 1)\n",
    "\n",
    "\n",
    "with open('out_generation.txt', 'w') as outf:\n",
    "    with torch.no_grad():  # no tracking history\n",
    "        for i in range(20):\n",
    "            output = model(input, False)\n",
    "            word_weights = output[-1].squeeze().div(temperature).exp().cpu()\n",
    "            word_idx = torch.multinomial(word_weights, 1)[0]\n",
    "            word_tensor = torch.Tensor([[word_idx]]).long().to(device)\n",
    "            print(word_tensor)\n",
    "            input = torch.cat([input, word_tensor], 0)\n",
    "\n",
    "            word = lookup_token(word_idx)\n",
    "\n",
    "            outf.write(word + ('\\n' if i % 20 == 19 else ' '))\n",
    "\n",
    "            if i % log_interval == 0:\n",
    "                print('| Generated {}/{} words'.format(i, [lookup_token(i[0])for i in input]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
