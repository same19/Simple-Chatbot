{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/word2vec-with-pytorch-implementing-original-paper-2cd7040120b0\n",
    "\n",
    "https://towardsdatascience.com/running-jupyter-notebook-on-the-cloud-in-15-mins-azure-79b7797e4ef6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maxtalwar/Simple-Chatbot/.venv/lib/python3.10/site-packages/torchtext/datasets/__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "/home/maxtalwar/Simple-Chatbot/.venv/lib/python3.10/site-packages/torchtext/data/__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchtext.datasets import WikiText2\n",
    "import pandas as pd\n",
    "from nltk.corpus import brown\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt\n",
    "import sys\n",
    "from sklearn.manifold import TSNE\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_to_array(t):\n",
    "    return np.array([i.item() for i in list(t)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2156"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "version = \"april29_WT2_nodatalim_20epoch_64dim_100minf_4window\"\n",
    "vocab = torch.load(\"saves/vocab_april27_WT2_nodatalim_10epoch_128dim_100minf.pt\")\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maxtalwar/Simple-Chatbot/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "wikitext2 = load_dataset(\"wikitext\", \"wikitext-2-v1\")\n",
    "DATA_SPLIT = \"train\"\n",
    "text = wikitext2[DATA_SPLIT]['text']\n",
    "text = [item.lower().strip() for item in text if len(item) > 0]\n",
    "len(text)\n",
    "text = [item.split(\" \") + [\"\\n\"] for item in text if \"=\" not in item]\n",
    "\n",
    "DATA_LIMIT = None #paragraph limit\n",
    "all_words = []\n",
    "for paragraph in text[:DATA_LIMIT]:\n",
    "    all_words += paragraph\n",
    "all_words = pd.Series(all_words)\n",
    "# len(all_words)\n",
    "\n",
    "print(sum(1 for i in all_words if i in vocab)/len(all_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCANNING_WINDOW = 4\n",
    "#maybe need to split into paragraphs b/c different topics...\n",
    "#returns context, middle word\n",
    "def get_data(index, window, data):\n",
    "    return list(data[index-window:index])+list(data[index+1:index+window+1]), data[index]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start here for training..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"train_data/\"\n",
    "version1 = \"_data_\"\n",
    "version2 = \"_wt2_window4_100minf.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = torch.load(f\"{folder}test{version1}x{version2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = torch.load(f\"{folder}test{version1}y{version2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.load(f\"{folder}train{version1}x{version2}\")\n",
    "len(x_train) + len(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = torch.load(f\"{folder}train{version1}y{version2}\")\n",
    "len(y_train) + len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from net import Net_CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_DIMENSION = 128\n",
    "net = Net_CBOW(len(vocab), EMBED_DIMENSION)\n",
    "\n",
    "net.zero_grad()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 20\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.025)\n",
    "scheduler = optim.lr_scheduler.LinearLR(optimizer, 1.0, 0.0, total_iters=NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"RUN       \" + (\"•••••••••|\"*10))\n",
    "indices = list(range(len(x_train)))\n",
    "torch.save(net, f\"saves/apr28epochs/model_{version}_init.pt\")\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(\"RUN\", str(epoch+1)+\"/\"+str(NUM_EPOCHS), end=\": \")\n",
    "    for i in range(len(x_train)):\n",
    "        if i % (len(x_train)//100) == 0:\n",
    "            print(\"•\", end=\"\")\n",
    "        index = indices[i]\n",
    "        context, target = x_train[index], y_train[index]\n",
    "        optimizer.zero_grad()   # zero the gradient buffers\n",
    "        output = net(torch.tensor(context))\n",
    "        loss = criterion(output, torch.tensor(target))\n",
    "        loss.backward()\n",
    "        optimizer.step()    # Does the update\n",
    "\n",
    "    for context, target in zip(x_test, y_test):\n",
    "        output = net(torch.tensor(context))\n",
    "        losses.append(criterion(output, torch.tensor(target)).item())\n",
    "    print(scheduler.get_last_lr())\n",
    "\n",
    "    scheduler.step()\n",
    "    print()\n",
    "    random.shuffle(indices)\n",
    "    torch.save(net, f\"saves/apr28epochs/model_{version}_epoch{str(epoch)}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(net, f\"saves/model_{version}.pt\")\n",
    "# torch.save(vocab, f\"saves/vocab_{version}.pt\")\n",
    "\n",
    "#Note that 4/26 20epoch version got to a loss of 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_per_epoch = []\n",
    "for i in range(NUM_EPOCHS):\n",
    "    loss_per_epoch += [sum(losses[(i)*len(x_test):(i+1)*len(x_test)])/len(x_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_per_epoch)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get first layer of the model\n",
    "embeddings = list(net.parameters())[0]\n",
    "embeddings = embeddings.cpu().detach().numpy()\n",
    "\n",
    "# normalize the embeddings layer\n",
    "norms = (embeddings ** 2).sum(axis=1) ** (0.5)\n",
    "norms = np.reshape(norms, (len(norms), 1))\n",
    "embeddings_norm = embeddings / norms\n",
    "embeddings_norm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_df = pd.DataFrame(embeddings)\n",
    "\n",
    "# t-SNE transform\n",
    "tsne = TSNE(n_components=2)\n",
    "embeddings_df_tsne = tsne.fit_transform(embeddings_df)\n",
    "embeddings_df_tsne = pd.DataFrame(embeddings_df_tsne)\n",
    "\n",
    "embeddings_df_tsne.index = vocab.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric = embeddings_df_tsne.index.str.isnumeric()\n",
    "color = np.where(numeric, \"green\", \"gray\")\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=embeddings_df_tsne[0],\n",
    "        y=embeddings_df_tsne[1],\n",
    "        mode=\"text\",\n",
    "        text=embeddings_df_tsne.index,\n",
    "        textposition=\"middle center\",\n",
    "        textfont=dict(color=color),\n",
    "    )\n",
    ")\n",
    "fig.write_html(\"word2vec_visualization.html\")\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lookup_id(word, vocab=vocab):\n",
    "    if word not in vocab:\n",
    "        return vocab[\"<unk>\"]\n",
    "    return vocab[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lookup_token(word_id, vocab=vocab):\n",
    "    for word in vocab:\n",
    "        if vocab[word] == word_id:\n",
    "            return word\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_similar(word: str, topN: int = 10):\n",
    "    if word not in vocab:\n",
    "        print(\"Out of vocabulary word\")\n",
    "        return\n",
    "    word_id = lookup_id(word)\n",
    "\n",
    "    word_vec = embeddings_norm[word_id]\n",
    "    word_vec = np.reshape(word_vec, (len(word_vec), 1))\n",
    "    dists = np.matmul(embeddings_norm, word_vec).flatten()\n",
    "    topN_ids = np.argsort(-dists)[1 : topN + 1]\n",
    "\n",
    "    topN_dict = {}\n",
    "    for sim_word_id in topN_ids:\n",
    "        # sim_word = vocab.lookup_token(sim_word_id)\n",
    "        sim_word = \"<unk_>\"\n",
    "        for k in vocab:\n",
    "            if vocab[k] == sim_word_id:\n",
    "                sim_word = k\n",
    "                break\n",
    "        topN_dict[sim_word] = dists[sim_word_id]\n",
    "    return topN_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word, sim in get_top_similar(\"1\").items():\n",
    "    print(\"{}: {:.3f}\".format(word, sim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb1 = embeddings[vocab[\"father\"]]\n",
    "emb2 = embeddings[vocab[\"man\"]]\n",
    "emb3 = embeddings[vocab[\"female\"]]\n",
    "\n",
    "emb4 = emb1 - emb2 + emb3\n",
    "emb4_norm = (emb4 ** 2).sum() ** (1 / 2)\n",
    "emb4 = emb4 / emb4_norm\n",
    "\n",
    "emb4 = np.reshape(emb4, (len(emb4), 1))\n",
    "dists = np.matmul(embeddings_norm, emb4).flatten()\n",
    "\n",
    "top5 = np.argsort(-dists)[:5]\n",
    "\n",
    "for word_id in top5:\n",
    "    print(\"{}: {:.3f}\".format(lookup_token(word_id), dists[word_id]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
